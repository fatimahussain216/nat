# -*- coding: utf-8 -*-
"""nat.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JnM6vBNm1_p7phbzgkinLRJ-UwR0d0Cm
"""

from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import numpy as np
import faiss
import streamlit as st
import requests
import json
import os
#upload file

def load_pdf(docs):
  loader = PyMuPDFLoader(docs)
  doc = loader.load()

  return doc


#split the text

def text_split(docs):
  text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=150,
       chunk_overlap=20,
       add_start_index=True
    )


  all_splits = text_splitter.split_documents(docs)
  return all_splits


#embedding the vector



def embeddings(chunks):
  filtered_texts = [chunk.page_content for chunk in chunks if len(chunk.page_content.strip()) > 20]
  emb = HuggingFaceEmbeddings(
          model_name="sentence-transformers/all-mpnet-base-v2",
          model_kwargs={"device": "cpu"},
          encode_kwargs={"normalize_embeddings": True}
      )
  vectors = emb.embed_documents([chunk.page_content for chunk in chunks])
  print(f"Original chunks: {len(chunks)}, Chunks embedded: {len(filtered_texts)}")

  return vectors, emb


#  load_faiss



def faiss_load(chunks, embeddings):
  embedding_matrix = np.array(embeddings, dtype='float32')
  dimension = embedding_matrix.shape[1]
  index = faiss.IndexFlatL2(dimension)
  index.add(embedding_matrix)

    #vectors = embeddings(chunks)
    #dimension = len(vectors[0])

  chunk_mapping = {i: chunks[i] for i in range(len(chunks))}

  print(f"FAISS index ready with {index.ntotal} vectors of dimension {dimension}")
  return index, chunk_mapping



#top retrieve

def retrieve(query, index, chunk_mapping, emb, k=3):
  # Embed query
  query_vec = emb.embed_query(query)
  query_vec = np.array(query_vec).astype('float32').reshape(1, -1)

  # Normalize for cosine similarity
  query_vec /= np.linalg.norm(query_vec)

  # FAISS search
  D, I = index.search(query_vec, k)

  # Map results
  results = [{"chunk": chunk_mapping[i], "score": float(D[0][j])} for j, i in enumerate(I[0])]
  return results




    #built prompt

def build_prompt(context_chunks, query):
  #
  # Har chunk ko check karein agar dict hai to 'page_content', warna string ke taur pe use karein
  context_list = []
  for chunk in context_chunks:
    if isinstance(chunk, dict) and 'page_content' in chunk:
      context_list.append(chunk['page_content'])
    else:
      # Agar chunk dict nahi ya key missing ho, to string representation use karein
      context_list.append(str(chunk))

  context = "\n\n".join(context_list)

  # Formatted prompt return karna
  return f"""use the following pieces of context to answer the question.
if you don't know the answer just say that you don't know, don't try to make up an answer.
and explain the answer of question in simple words.
context: {context}
question: {query}
answer:"""


    # completion
def generate_completion(prompt, model="gpt-4.1-nano"):
    url = "https://api.euron.one/api/v1/euri/chat/completions"
    api_key="api_key"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "messages":[{"role":"user","content":prompt}],
               "max_tokens":110,"temperature":0.4}
    response = requests.post(url, headers=headers, json=payload)

    data = response.json()


    return data['choices'][0]['message']['content']




    # built streamlit
st.title("RAG App - about PDFüìÑ")
st.header("RAG Application üïµÔ∏è‚Äç‚ôÄÔ∏è")
st.write("Ask any question about pdf")
st.subheader("Upload PDF")

docs = st.file_uploader("Upload PDF", type=["pdf"])
if docs is None:
    st.error("Please upload a PDF file")
    st.stop()

st.subheader("Enter Your Question")
query = st.text_input("Enter Your Question")

if query:
    with st.spinner("Processing..."):

      with open("temp.pdf", "wb") as f:
        f.write(docs.getbuffer())
        loader = PyMuPDFLoader("temp.pdf")
        load_pdf = loader.load()
        # Ab file ka path PyMuPDFLoader ko do


    #st.write(f"Loaded {len(docs)} pages from the PDF.")
    chunks = text_split(load_pdf)
    vectors, emb_model = embeddings(chunks)
    index, chunk_mapping = faiss_load(chunks, vectors)
    top_chunks = retrieve(query, index, chunk_mapping,emb_model, k=3)

    prompt = build_prompt(top_chunks, query)
    assistant_reply = generate_completion(prompt)
    results = retrieve(query, index, chunk_mapping, emb_model, k=2)

    unique_chunks = list({item['chunk'].page_content for item in results})
    answer = " ".join(unique_chunks)

    st.subheader("Answer")
    st.write(answer)

    st.subheader("Top Chunks")
    st.write(top_chunks)

    with st.expander("Retrieved Chunks"):
      for c in top_chunks:
        st.markdown(f"- {c}")